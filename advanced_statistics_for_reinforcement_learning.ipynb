{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12f4767",
   "metadata": {},
   "source": [
    "# Advanced Statistics for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76450c13",
   "metadata": {},
   "source": [
    "## 1. Markov Decision Processes (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c5a7a",
   "metadata": {},
   "source": [
    "\n",
    "### Definition:\n",
    "- A mathematical framework for modeling decision-making where outcomes are partly random and partly under the control of a decision maker.\n",
    "\n",
    "### Components:\n",
    "- **States (S)**: All possible situations in which the agent can be.\n",
    "- **Actions (A)**: All possible actions the agent can take.\n",
    "- **Transition Model (T)**: Describes the probability of transitioning from one state to another given an action.\n",
    "- **Reward Function (R)**: Provides feedback to the agent about the immediate benefit of taking an action in a state.\n",
    "- **Discount Factor (γ)**: Determines the importance of future rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50898c",
   "metadata": {},
   "source": [
    "## 2. Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ae0c0",
   "metadata": {},
   "source": [
    "\n",
    "### Concepts:\n",
    "- **State Value Function (V)**: The expected return when starting from state \\( s \\) and following a policy \\( π \\).\n",
    "- **Action Value Function (Q)**: The expected return when taking action \\( a \\) in state \\( s \\) and then following policy \\( π \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03607aeb",
   "metadata": {},
   "source": [
    "## 3. Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fa72d",
   "metadata": {},
   "source": [
    "\n",
    "### Concepts:\n",
    "- **Policy (π)**: A strategy that the agent employs to determine its actions based on the current state.\n",
    "- **Exploration vs. Exploitation**: Balancing the need to explore the environment versus exploiting known strategies to maximize rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58761efe",
   "metadata": {},
   "source": [
    "## 4. Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95b7c4",
   "metadata": {},
   "source": [
    "\n",
    "### Techniques:\n",
    "- **Q-Learning**: A model-free reinforcement learning algorithm that learns the value of action in a particular state without needing a model of the environment.\n",
    "- **SARSA (State-Action-Reward-State-Action)**: An on-policy learning algorithm that updates Q-values based on the action taken by the current policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1f9e4",
   "metadata": {},
   "source": [
    "## 5. Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ff0b2",
   "metadata": {},
   "source": [
    "\n",
    "### Definition:\n",
    "- Algorithms that rely on repeated random sampling to obtain numerical results, used in RL for estimating value functions and improving policies.\n",
    "\n",
    "### Returns:\n",
    "- The total discounted reward from a given time step until the end of an episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb59b6",
   "metadata": {},
   "source": [
    "## 6. Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047f3b9",
   "metadata": {},
   "source": [
    "\n",
    "### Techniques:\n",
    "- **Definition**: Techniques that optimize the policy directly rather than the value function.\n",
    "- **REINFORCE Algorithm**: A policy gradient method that uses Monte Carlo returns to update the policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791991d",
   "metadata": {},
   "source": [
    "## 7. Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2f9e3",
   "metadata": {},
   "source": [
    "\n",
    "### Definition:\n",
    "- Combines value function approximation (critic) and policy gradient (actor) to improve learning efficiency.\n",
    "\n",
    "### Advantage Function:\n",
    "- Used to reduce the variance of the policy gradient estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2a8f4",
   "metadata": {},
   "source": [
    "## 8. Exploration Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec81ce2",
   "metadata": {},
   "source": [
    "\n",
    "### Techniques:\n",
    "- **Epsilon-Greedy**: With probability \\( ε \\), take a random action instead of the best-known action.\n",
    "- **Softmax Action Selection**: Actions are selected based on a softmax distribution over Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f963101",
   "metadata": {},
   "source": [
    "## 9. Temporal Logic in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e906c4b",
   "metadata": {},
   "source": [
    "\n",
    "### Concepts:\n",
    "- **Linear Temporal Logic (LTL)**: Used to specify desired properties in the behavior of the agent over time.\n",
    "- **Reward Shaping**: Modifying the reward structure to encourage specific behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7a729",
   "metadata": {},
   "source": [
    "## 10. Advanced Topics in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc43cae",
   "metadata": {},
   "source": [
    "\n",
    "### Topics:\n",
    "- **Hierarchical Reinforcement Learning**: Breaking down tasks into smaller subtasks.\n",
    "- **Multi-Agent Reinforcement Learning**: Learning in environments with multiple interacting agents.\n",
    "- **Transfer Learning in RL**: Applying knowledge gained in one task to improve learning in a related task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbadbd9",
   "metadata": {},
   "source": [
    "## 11. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e6dc6",
   "metadata": {},
   "source": [
    "\n",
    "### Metrics:\n",
    "- **Cumulative Reward**: Total reward received over an episode.\n",
    "- **Average Reward**: Average reward per time step over episodes.\n",
    "- **Convergence**: The process of learning stabilizing to a point where further learning has minimal impact.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
