{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f7e87d",
   "metadata": {},
   "source": [
    "# Supervised Learning Algorithms Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08330ca",
   "metadata": {},
   "source": [
    "## 1. Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4321b",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Logistic Regression\n",
    "- A statistical method for binary classification that predicts probabilities of class membership based on a logistic function.\n",
    "- **Use Case**: Email spam detection (spam vs. not spam).\n",
    "\n",
    "### 1.2 Linear Discriminant Analysis (LDA)\n",
    "- A method used to find a linear combination of features that separates two or more classes.\n",
    "- **Use Case**: Face recognition.\n",
    "\n",
    "### 1.3 Decision Trees\n",
    "- A tree-like model where internal nodes represent feature tests, branches represent outcomes, and leaves represent class labels.\n",
    "- **Use Case**: Customer segmentation.\n",
    "\n",
    "### 1.4 Random Forest\n",
    "- An ensemble method using multiple decision trees for improved accuracy and robustness through averaging predictions.\n",
    "- **Use Case**: Predicting loan defaults.\n",
    "\n",
    "### 1.5 Gradient Boosting Machines (GBM)\n",
    "- Builds trees sequentially, where each new tree corrects the errors made by the previous trees.\n",
    "- **Use Case**: Predicting house prices.\n",
    "\n",
    "### 1.6 Support Vector Machines (SVM)\n",
    "- A powerful classification algorithm that finds the optimal hyperplane that separates classes in feature space.\n",
    "- **Use Case**: Handwritten digit recognition.\n",
    "\n",
    "### 1.7 k-Nearest Neighbors (k-NN)\n",
    "- A non-parametric method that classifies instances based on the majority class of their k-nearest neighbors in the feature space.\n",
    "- **Use Case**: Image classification.\n",
    "\n",
    "### 1.8 Naive Bayes\n",
    "- A family of probabilistic classifiers based on Bayes' theorem, assuming independence between features.\n",
    "- **Use Case**: Document categorization.\n",
    "\n",
    "### 1.9 Neural Networks\n",
    "- Used for complex classification tasks, mimicking the structure and functioning of the human brain.\n",
    "- **Use Case**: Image and speech recognition.\n",
    "\n",
    "### 1.10 AdaBoost\n",
    "- An ensemble technique that combines the predictions of several weak learners to create a strong learner.\n",
    "- **Use Case**: Object detection in images.\n",
    "\n",
    "### 1.11 Quadratic Discriminant Analysis (QDA)\n",
    "- A classification technique that generalizes LDA by allowing different covariance matrices for each class.\n",
    "- **Use Case**: Classifying different types of plants based on features.\n",
    "\n",
    "### 1.12 Extreme Gradient Boosting (XGBoost)\n",
    "- An optimized gradient boosting algorithm known for its speed and performance.\n",
    "- **Use Case**: Kaggle competitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83811633",
   "metadata": {},
   "source": [
    "## 2. Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26730d47",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Linear Regression\n",
    "- A method that models the relationship between a dependent variable and one or more independent variables using a linear equation.\n",
    "- **Use Case**: Predicting sales based on advertising spend.\n",
    "\n",
    "### 2.2 Ridge Regression\n",
    "- A regularization technique that adds a penalty equal to the square of the magnitude of coefficients to the loss function.\n",
    "- **Use Case**: Preventing overfitting in high-dimensional datasets.\n",
    "\n",
    "### 2.3 Lasso Regression\n",
    "- Adds a penalty equal to the absolute value of the magnitude of coefficients, promoting sparsity in the model.\n",
    "- **Use Case**: Feature selection in regression models.\n",
    "\n",
    "### 2.4 Elastic Net\n",
    "- Combines both Lasso and Ridge regression penalties.\n",
    "- **Use Case**: When there are multiple features correlated with each other.\n",
    "\n",
    "### 2.5 Decision Trees for Regression\n",
    "- Similar to classification trees but used for predicting continuous outcomes.\n",
    "- **Use Case**: Predicting house prices based on features.\n",
    "\n",
    "### 2.6 Random Forest for Regression\n",
    "- An ensemble of decision trees used for regression tasks, averaging the outputs of multiple trees.\n",
    "- **Use Case**: Stock price prediction.\n",
    "\n",
    "### 2.7 Gradient Boosting for Regression\n",
    "- Builds trees sequentially for regression tasks, correcting the errors of previous trees.\n",
    "- **Use Case**: Energy consumption prediction.\n",
    "\n",
    "### 2.8 Support Vector Regression (SVR)\n",
    "- A regression version of SVM that aims to minimize error within a certain threshold.\n",
    "- **Use Case**: Predicting real estate prices.\n",
    "\n",
    "### 2.9 k-Nearest Neighbors for Regression\n",
    "- Predicts the output of a data point based on the average of its k-nearest neighbors.\n",
    "- **Use Case**: Estimating the price of a used car based on similar car sales.\n",
    "\n",
    "### 2.10 Neural Networks for Regression\n",
    "- Can be used to model complex relationships in continuous target variables.\n",
    "- **Use Case**: Predicting temperature based on historical data.\n",
    "\n",
    "### 2.11 Polynomial Regression\n",
    "- Extends linear regression by fitting a polynomial equation to the data.\n",
    "- **Use Case**: Modeling non-linear relationships in datasets.\n",
    "\n",
    "### 2.12 Quantile Regression\n",
    "- Predicts the conditional quantiles of the target variable, providing a more comprehensive view of the relationship.\n",
    "- **Use Case**: Estimating different percentiles of house prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248be164",
   "metadata": {},
   "source": [
    "## 3. Other Notable Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730ebff",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Multi-Class Support Vector Machines\n",
    "- An extension of SVM for multi-class classification problems.\n",
    "\n",
    "### 3.2 Bayesian Networks\n",
    "- A graphical model that represents the conditional dependencies between random variables.\n",
    "\n",
    "### 3.3 Multi-Task Learning Models\n",
    "- Models that learn from multiple related tasks simultaneously, sharing representations across tasks.\n",
    "\n",
    "### 3.4 Semi-Supervised Learning Algorithms\n",
    "- Combines a small amount of labeled data with a large amount of unlabeled data to improve learning performance.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
